{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# GENERAL: Strategic Game AI Training\n",
                "\n",
                "GPU-accelerated training using AlphaZero-style RL. Optimizations enabled for Colab (T4) and Local (RTX) environments.\n",
                "\n",
                "This notebook uses the optimized `GenGameAI` codebase with:\n",
                "- **Non-blocking Inference** (Asyncio + ThreadPool)\n",
                "- **Efficient Data Loading** (DataLoader with pinned memory)\n",
                "- **Auto-Configuration** (Detects GPU VRAM and CPU cores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "!git clone https://github.com/Tanish-2006/Generals.git\n",
                "%cd Generals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "!python3.11 -m pip install torch numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "e:\\Projects\\GenGameAI\n"
                    ]
                }
            ],
            "source": [
                "%cd .."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config_header"
            },
            "source": [
                "## 2. Configuration & Hardware Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "verify_config"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
                        "VRAM: 8.59 GB\n",
                        "\n",
                        "Auto-Detected Configuration:\n",
                        "  Workers: 0\n",
                        "  Batch Size: 64\n",
                        "  Games/Iter: 32\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "import torch\n",
                "from config import TRAINING\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected.\")\n",
                "\n",
                "print(\"\\nAuto-Detected Configuration:\")\n",
                "print(f\"  Workers: {TRAINING.num_workers}\")\n",
                "print(f\"  Batch Size: {TRAINING.batch_size}\")\n",
                "print(f\"  Games/Iter: {TRAINING.games_per_iter}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "upload_header"
            },
            "source": [
                "## 3. Upload Previous Model (Optional)\n",
                "If you have a `model_latest.pth` or `model_old.pth` from a previous run, upload it here to resume training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "upload_model"
            },
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'google.colab'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m CHECKPOINT_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33mdata/checkpoints\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
                    ]
                }
            ],
            "source": [
                "from google.colab import files\n",
                "from pathlib import Path\n",
                "\n",
                "CHECKPOINT_DIR = Path(\"data/checkpoints\")\n",
                "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"Upload model_latest.pth or model_old.pth if you have one:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    target_path = CHECKPOINT_DIR / filename\n",
                "    with open(target_path, 'wb') as f:\n",
                "        f.write(uploaded[filename])\n",
                "    print(f\"Saved {filename} to {target_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training_header"
            },
            "source": [
                "## 4. Training Loop\n",
                "Runs the main optimized training loop. Supports resuming if models exist."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_training"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[main] Found 23 replay batches. Resuming from iteration 24.\n",
                        "[Trainer] Using device: cuda\n",
                        "[Trainer] AMP enabled for faster training\n",
                        "[Trainer] JIT compilation skipped on Windows (Triton not supported)\n",
                        "[main] Resuming with Best Model (model_old.pth)\n",
                        "[main] Successfully loaded model from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_old.pth\n",
                        "[InferenceServer] Started on cuda:0 with batch_size=32\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 24 - self-play 32 games\n",
                        "============================================================\n",
                        "[main] Generating 32 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0024.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 24 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 46415 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/725 | Loss: 1.5234 | Policy: 1.1100 | Value: 0.4134\n",
                        "  Batch  20/725 | Loss: 2.0458 | Policy: 1.6361 | Value: 0.4098\n",
                        "  Batch  40/725 | Loss: 1.6212 | Policy: 1.2416 | Value: 0.3797\n",
                        "  Batch  60/725 | Loss: 1.6172 | Policy: 1.3420 | Value: 0.2752\n",
                        "  Batch  80/725 | Loss: 1.6813 | Policy: 1.0714 | Value: 0.6099\n",
                        "  Batch 100/725 | Loss: 1.7401 | Policy: 1.5352 | Value: 0.2049\n",
                        "  Batch 120/725 | Loss: 1.6072 | Policy: 1.2466 | Value: 0.3606\n",
                        "  Batch 140/725 | Loss: 1.6279 | Policy: 1.1131 | Value: 0.5148\n",
                        "  Batch 160/725 | Loss: 1.9618 | Policy: 1.6024 | Value: 0.3594\n",
                        "  Batch 180/725 | Loss: 1.3637 | Policy: 1.0099 | Value: 0.3538\n",
                        "  Batch 200/725 | Loss: 1.5667 | Policy: 1.3528 | Value: 0.2139\n",
                        "  Batch 220/725 | Loss: 1.7412 | Policy: 1.4246 | Value: 0.3166\n",
                        "  Batch 240/725 | Loss: 1.4322 | Policy: 1.1312 | Value: 0.3010\n",
                        "  Batch 260/725 | Loss: 1.9460 | Policy: 1.6090 | Value: 0.3369\n",
                        "  Batch 280/725 | Loss: 1.2490 | Policy: 0.9190 | Value: 0.3301\n",
                        "  Batch 300/725 | Loss: 1.9167 | Policy: 1.5799 | Value: 0.3368\n",
                        "  Batch 320/725 | Loss: 1.8264 | Policy: 1.4294 | Value: 0.3970\n",
                        "  Batch 340/725 | Loss: 1.7537 | Policy: 1.3128 | Value: 0.4409\n",
                        "  Batch 360/725 | Loss: 1.7054 | Policy: 1.3415 | Value: 0.3640\n",
                        "  Batch 380/725 | Loss: 1.3484 | Policy: 1.1847 | Value: 0.1636\n",
                        "  Batch 400/725 | Loss: 2.0956 | Policy: 1.6892 | Value: 0.4064\n",
                        "  Batch 420/725 | Loss: 1.9579 | Policy: 1.6513 | Value: 0.3066\n",
                        "  Batch 440/725 | Loss: 2.2819 | Policy: 1.7260 | Value: 0.5559\n",
                        "  Batch 460/725 | Loss: 1.4159 | Policy: 1.2015 | Value: 0.2144\n",
                        "  Batch 480/725 | Loss: 1.4224 | Policy: 1.1907 | Value: 0.2317\n",
                        "  Batch 500/725 | Loss: 1.6659 | Policy: 1.2624 | Value: 0.4035\n",
                        "  Batch 520/725 | Loss: 2.0967 | Policy: 1.4653 | Value: 0.6314\n",
                        "  Batch 540/725 | Loss: 1.7420 | Policy: 1.5157 | Value: 0.2263\n",
                        "  Batch 560/725 | Loss: 2.0864 | Policy: 1.7004 | Value: 0.3860\n",
                        "  Batch 580/725 | Loss: 1.6970 | Policy: 1.5110 | Value: 0.1859\n",
                        "  Batch 600/725 | Loss: 1.6609 | Policy: 1.2502 | Value: 0.4107\n",
                        "  Batch 620/725 | Loss: 1.3059 | Policy: 1.0568 | Value: 0.2491\n",
                        "  Batch 640/725 | Loss: 1.8326 | Policy: 1.5055 | Value: 0.3271\n",
                        "  Batch 660/725 | Loss: 1.6992 | Policy: 1.3832 | Value: 0.3160\n",
                        "  Batch 680/725 | Loss: 1.6193 | Policy: 1.3226 | Value: 0.2967\n",
                        "  Batch 700/725 | Loss: 2.0511 | Policy: 1.5956 | Value: 0.4554\n",
                        "  Batch 720/725 | Loss: 1.1861 | Policy: 1.0409 | Value: 0.1451\n",
                        "\n",
                        "Epoch 1 Average Loss: 1.7108\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/725 | Loss: 1.1309 | Policy: 0.8446 | Value: 0.2863\n",
                        "  Batch  20/725 | Loss: 1.0637 | Policy: 0.8605 | Value: 0.2032\n",
                        "  Batch  40/725 | Loss: 1.3282 | Policy: 1.0899 | Value: 0.2384\n",
                        "  Batch  60/725 | Loss: 1.2586 | Policy: 1.0501 | Value: 0.2085\n",
                        "  Batch  80/725 | Loss: 1.3490 | Policy: 1.0859 | Value: 0.2631\n",
                        "  Batch 100/725 | Loss: 1.4000 | Policy: 1.1100 | Value: 0.2900\n",
                        "  Batch 120/725 | Loss: 0.9324 | Policy: 0.8120 | Value: 0.1205\n",
                        "  Batch 140/725 | Loss: 1.3838 | Policy: 1.2500 | Value: 0.1338\n",
                        "  Batch 160/725 | Loss: 1.0751 | Policy: 0.9370 | Value: 0.1381\n",
                        "  Batch 180/725 | Loss: 0.9660 | Policy: 0.7271 | Value: 0.2389\n",
                        "  Batch 200/725 | Loss: 1.0997 | Policy: 0.9396 | Value: 0.1602\n",
                        "  Batch 220/725 | Loss: 1.0712 | Policy: 0.7336 | Value: 0.3376\n",
                        "  Batch 240/725 | Loss: 1.3803 | Policy: 1.2182 | Value: 0.1620\n",
                        "  Batch 260/725 | Loss: 1.1601 | Policy: 0.7754 | Value: 0.3847\n",
                        "  Batch 280/725 | Loss: 0.9573 | Policy: 0.8720 | Value: 0.0854\n",
                        "  Batch 300/725 | Loss: 1.2917 | Policy: 0.9911 | Value: 0.3006\n",
                        "  Batch 320/725 | Loss: 1.6032 | Policy: 1.3370 | Value: 0.2662\n",
                        "  Batch 340/725 | Loss: 1.2059 | Policy: 0.9163 | Value: 0.2896\n",
                        "  Batch 360/725 | Loss: 1.7892 | Policy: 1.4703 | Value: 0.3189\n",
                        "  Batch 380/725 | Loss: 1.4029 | Policy: 1.1518 | Value: 0.2511\n",
                        "  Batch 400/725 | Loss: 1.6908 | Policy: 1.4684 | Value: 0.2224\n",
                        "  Batch 420/725 | Loss: 1.4613 | Policy: 1.0796 | Value: 0.3817\n",
                        "  Batch 440/725 | Loss: 1.0632 | Policy: 0.9213 | Value: 0.1419\n",
                        "  Batch 460/725 | Loss: 1.5311 | Policy: 1.2720 | Value: 0.2591\n",
                        "  Batch 480/725 | Loss: 2.2397 | Policy: 1.4917 | Value: 0.7480\n",
                        "  Batch 500/725 | Loss: 1.2457 | Policy: 0.9191 | Value: 0.3267\n",
                        "  Batch 520/725 | Loss: 1.1718 | Policy: 0.9808 | Value: 0.1910\n",
                        "  Batch 540/725 | Loss: 1.1550 | Policy: 0.9998 | Value: 0.1551\n",
                        "  Batch 560/725 | Loss: 1.0724 | Policy: 0.9492 | Value: 0.1232\n",
                        "  Batch 580/725 | Loss: 1.3919 | Policy: 1.2495 | Value: 0.1424\n",
                        "  Batch 600/725 | Loss: 1.3157 | Policy: 1.0347 | Value: 0.2810\n",
                        "  Batch 620/725 | Loss: 1.2912 | Policy: 1.1760 | Value: 0.1152\n",
                        "  Batch 640/725 | Loss: 1.5500 | Policy: 1.3507 | Value: 0.1993\n",
                        "  Batch 660/725 | Loss: 1.2688 | Policy: 0.9961 | Value: 0.2728\n",
                        "  Batch 680/725 | Loss: 1.3886 | Policy: 1.1353 | Value: 0.2534\n",
                        "  Batch 700/725 | Loss: 1.6186 | Policy: 1.2858 | Value: 0.3328\n",
                        "  Batch 720/725 | Loss: 1.4583 | Policy: 1.2167 | Value: 0.2416\n",
                        "\n",
                        "Epoch 2 Average Loss: 1.2943\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/725 | Loss: 1.0879 | Policy: 0.9383 | Value: 0.1496\n",
                        "  Batch  20/725 | Loss: 1.0311 | Policy: 0.8449 | Value: 0.1862\n",
                        "  Batch  40/725 | Loss: 0.8936 | Policy: 0.7800 | Value: 0.1136\n",
                        "  Batch  60/725 | Loss: 0.8867 | Policy: 0.8242 | Value: 0.0625\n",
                        "  Batch  80/725 | Loss: 1.0717 | Policy: 0.9596 | Value: 0.1121\n",
                        "  Batch 100/725 | Loss: 0.7603 | Policy: 0.6187 | Value: 0.1416\n",
                        "  Batch 120/725 | Loss: 0.7354 | Policy: 0.6277 | Value: 0.1076\n",
                        "  Batch 140/725 | Loss: 0.9465 | Policy: 0.6919 | Value: 0.2546\n",
                        "  Batch 160/725 | Loss: 0.9547 | Policy: 0.7173 | Value: 0.2374\n",
                        "  Batch 180/725 | Loss: 0.7089 | Policy: 0.6719 | Value: 0.0370\n",
                        "  Batch 200/725 | Loss: 1.0165 | Policy: 0.7653 | Value: 0.2512\n",
                        "  Batch 220/725 | Loss: 1.1770 | Policy: 0.8903 | Value: 0.2867\n",
                        "  Batch 240/725 | Loss: 0.7451 | Policy: 0.5573 | Value: 0.1878\n",
                        "  Batch 260/725 | Loss: 1.1534 | Policy: 1.0145 | Value: 0.1388\n",
                        "  Batch 280/725 | Loss: 0.9055 | Policy: 0.7803 | Value: 0.1252\n",
                        "  Batch 300/725 | Loss: 0.7997 | Policy: 0.6909 | Value: 0.1089\n",
                        "  Batch 320/725 | Loss: 1.0258 | Policy: 0.9172 | Value: 0.1086\n",
                        "  Batch 340/725 | Loss: 1.1291 | Policy: 0.9178 | Value: 0.2113\n",
                        "  Batch 360/725 | Loss: 1.2733 | Policy: 0.9245 | Value: 0.3488\n",
                        "  Batch 380/725 | Loss: 0.7705 | Policy: 0.6545 | Value: 0.1160\n",
                        "  Batch 400/725 | Loss: 0.8835 | Policy: 0.8095 | Value: 0.0740\n",
                        "  Batch 420/725 | Loss: 1.1550 | Policy: 0.7840 | Value: 0.3710\n",
                        "  Batch 440/725 | Loss: 0.9223 | Policy: 0.7486 | Value: 0.1737\n",
                        "  Batch 460/725 | Loss: 0.9803 | Policy: 0.7955 | Value: 0.1848\n",
                        "  Batch 480/725 | Loss: 1.1780 | Policy: 0.9799 | Value: 0.1981\n",
                        "  Batch 500/725 | Loss: 0.9709 | Policy: 0.9005 | Value: 0.0704\n",
                        "  Batch 520/725 | Loss: 0.7868 | Policy: 0.6509 | Value: 0.1359\n",
                        "  Batch 540/725 | Loss: 1.2441 | Policy: 1.0637 | Value: 0.1804\n",
                        "  Batch 560/725 | Loss: 1.0690 | Policy: 0.8581 | Value: 0.2108\n",
                        "  Batch 580/725 | Loss: 1.0885 | Policy: 0.8272 | Value: 0.2612\n",
                        "  Batch 600/725 | Loss: 1.2917 | Policy: 1.0798 | Value: 0.2119\n",
                        "  Batch 620/725 | Loss: 1.0326 | Policy: 0.7869 | Value: 0.2458\n",
                        "  Batch 640/725 | Loss: 1.0579 | Policy: 0.9516 | Value: 0.1064\n",
                        "  Batch 660/725 | Loss: 1.0673 | Policy: 0.9208 | Value: 0.1465\n",
                        "  Batch 680/725 | Loss: 1.0882 | Policy: 0.9437 | Value: 0.1445\n",
                        "  Batch 700/725 | Loss: 1.2986 | Policy: 0.9960 | Value: 0.3025\n",
                        "  Batch 720/725 | Loss: 0.8932 | Policy: 0.7809 | Value: 0.1123\n",
                        "\n",
                        "Epoch 3 Average Loss: 1.0450\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: A\n",
                        "Game 2/10 → Winner: B\n",
                        "Game 3/10 → Winner: A\n",
                        "Game 4/10 → Winner: A\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: A\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: B\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 7\n",
                        "Model B wins: 3\n",
                        "Win rate A: 0.70\n",
                        "[main] Arena win rate for new model: 0.70\n",
                        "[main] New model accepted! Copying model_latest -> model_old\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 25 - self-play 32 games\n",
                        "============================================================\n",
                        "[main] Generating 32 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0025.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 25 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 49779 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/777 | Loss: 0.9299 | Policy: 0.5355 | Value: 0.3944\n",
                        "  Batch  20/777 | Loss: 0.6877 | Policy: 0.5735 | Value: 0.1143\n",
                        "  Batch  40/777 | Loss: 0.8990 | Policy: 0.6187 | Value: 0.2802\n",
                        "  Batch  60/777 | Loss: 0.9165 | Policy: 0.6862 | Value: 0.2303\n",
                        "  Batch  80/777 | Loss: 1.0199 | Policy: 0.8495 | Value: 0.1703\n",
                        "  Batch 100/777 | Loss: 1.1501 | Policy: 0.7585 | Value: 0.3916\n",
                        "  Batch 120/777 | Loss: 1.2330 | Policy: 1.1433 | Value: 0.0898\n",
                        "  Batch 140/777 | Loss: 1.4248 | Policy: 0.9995 | Value: 0.4253\n",
                        "  Batch 160/777 | Loss: 0.9375 | Policy: 0.7238 | Value: 0.2137\n",
                        "  Batch 180/777 | Loss: 1.2875 | Policy: 1.1013 | Value: 0.1862\n",
                        "  Batch 200/777 | Loss: 1.2744 | Policy: 1.0760 | Value: 0.1984\n",
                        "  Batch 220/777 | Loss: 1.0666 | Policy: 0.8200 | Value: 0.2466\n",
                        "  Batch 240/777 | Loss: 1.1449 | Policy: 1.0051 | Value: 0.1398\n",
                        "  Batch 260/777 | Loss: 1.4122 | Policy: 1.1545 | Value: 0.2577\n",
                        "  Batch 280/777 | Loss: 1.3113 | Policy: 1.0465 | Value: 0.2648\n",
                        "  Batch 300/777 | Loss: 1.4167 | Policy: 1.0491 | Value: 0.3676\n",
                        "  Batch 320/777 | Loss: 1.2095 | Policy: 0.7910 | Value: 0.4185\n",
                        "  Batch 340/777 | Loss: 1.1319 | Policy: 0.9279 | Value: 0.2040\n",
                        "  Batch 360/777 | Loss: 1.1618 | Policy: 1.0834 | Value: 0.0784\n",
                        "  Batch 380/777 | Loss: 1.1764 | Policy: 0.9167 | Value: 0.2596\n",
                        "  Batch 400/777 | Loss: 1.0962 | Policy: 0.7236 | Value: 0.3727\n",
                        "  Batch 420/777 | Loss: 1.0467 | Policy: 0.8014 | Value: 0.2453\n",
                        "  Batch 440/777 | Loss: 1.5731 | Policy: 1.2308 | Value: 0.3424\n",
                        "  Batch 460/777 | Loss: 1.1512 | Policy: 0.8557 | Value: 0.2956\n",
                        "  Batch 480/777 | Loss: 0.7697 | Policy: 0.5242 | Value: 0.2455\n",
                        "  Batch 500/777 | Loss: 1.2177 | Policy: 0.9496 | Value: 0.2680\n",
                        "  Batch 520/777 | Loss: 0.9274 | Policy: 0.8368 | Value: 0.0906\n",
                        "  Batch 540/777 | Loss: 1.0173 | Policy: 0.7592 | Value: 0.2581\n",
                        "  Batch 560/777 | Loss: 1.7881 | Policy: 1.4755 | Value: 0.3125\n",
                        "  Batch 580/777 | Loss: 1.0675 | Policy: 0.8657 | Value: 0.2018\n",
                        "  Batch 600/777 | Loss: 1.1019 | Policy: 0.8577 | Value: 0.2442\n",
                        "  Batch 620/777 | Loss: 1.2598 | Policy: 1.0477 | Value: 0.2120\n",
                        "  Batch 640/777 | Loss: 0.6791 | Policy: 0.4826 | Value: 0.1965\n",
                        "  Batch 660/777 | Loss: 0.6434 | Policy: 0.5373 | Value: 0.1061\n",
                        "  Batch 680/777 | Loss: 1.3480 | Policy: 1.1970 | Value: 0.1510\n",
                        "  Batch 700/777 | Loss: 1.1007 | Policy: 0.8421 | Value: 0.2585\n",
                        "  Batch 720/777 | Loss: 1.6296 | Policy: 1.3827 | Value: 0.2469\n",
                        "  Batch 740/777 | Loss: 1.4095 | Policy: 1.1632 | Value: 0.2462\n",
                        "  Batch 760/777 | Loss: 1.0976 | Policy: 0.7738 | Value: 0.3238\n",
                        "\n",
                        "Epoch 1 Average Loss: 1.1277\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/777 | Loss: 0.8131 | Policy: 0.5793 | Value: 0.2338\n",
                        "  Batch  20/777 | Loss: 0.8082 | Policy: 0.6602 | Value: 0.1480\n",
                        "  Batch  40/777 | Loss: 0.5582 | Policy: 0.4728 | Value: 0.0855\n",
                        "  Batch  60/777 | Loss: 0.7738 | Policy: 0.6195 | Value: 0.1543\n",
                        "  Batch  80/777 | Loss: 0.6119 | Policy: 0.4184 | Value: 0.1935\n",
                        "  Batch 100/777 | Loss: 0.8411 | Policy: 0.6739 | Value: 0.1672\n",
                        "  Batch 120/777 | Loss: 1.1221 | Policy: 0.8549 | Value: 0.2672\n",
                        "  Batch 140/777 | Loss: 0.7462 | Policy: 0.4549 | Value: 0.2912\n",
                        "  Batch 160/777 | Loss: 0.6017 | Policy: 0.4833 | Value: 0.1184\n",
                        "  Batch 180/777 | Loss: 0.7695 | Policy: 0.5789 | Value: 0.1907\n",
                        "  Batch 200/777 | Loss: 0.9749 | Policy: 0.6242 | Value: 0.3506\n",
                        "  Batch 220/777 | Loss: 0.6725 | Policy: 0.5321 | Value: 0.1404\n",
                        "  Batch 240/777 | Loss: 0.7070 | Policy: 0.6038 | Value: 0.1032\n",
                        "  Batch 260/777 | Loss: 0.8838 | Policy: 0.6744 | Value: 0.2094\n",
                        "  Batch 280/777 | Loss: 0.7771 | Policy: 0.4700 | Value: 0.3071\n",
                        "  Batch 300/777 | Loss: 1.1497 | Policy: 0.9743 | Value: 0.1754\n",
                        "  Batch 320/777 | Loss: 1.0255 | Policy: 0.8530 | Value: 0.1726\n",
                        "  Batch 340/777 | Loss: 1.0997 | Policy: 0.8775 | Value: 0.2222\n",
                        "  Batch 360/777 | Loss: 0.8088 | Policy: 0.6165 | Value: 0.1923\n",
                        "  Batch 380/777 | Loss: 1.1326 | Policy: 0.9481 | Value: 0.1845\n",
                        "  Batch 400/777 | Loss: 0.8327 | Policy: 0.7301 | Value: 0.1026\n",
                        "  Batch 420/777 | Loss: 0.8997 | Policy: 0.7360 | Value: 0.1637\n",
                        "  Batch 440/777 | Loss: 0.7337 | Policy: 0.6474 | Value: 0.0863\n",
                        "  Batch 460/777 | Loss: 0.7543 | Policy: 0.5795 | Value: 0.1748\n",
                        "  Batch 480/777 | Loss: 0.7223 | Policy: 0.5470 | Value: 0.1753\n",
                        "  Batch 500/777 | Loss: 0.5545 | Policy: 0.4210 | Value: 0.1335\n",
                        "  Batch 520/777 | Loss: 1.1390 | Policy: 0.8715 | Value: 0.2675\n",
                        "  Batch 540/777 | Loss: 1.0886 | Policy: 0.9121 | Value: 0.1765\n",
                        "  Batch 560/777 | Loss: 1.1431 | Policy: 0.8027 | Value: 0.3404\n",
                        "  Batch 580/777 | Loss: 0.8948 | Policy: 0.6974 | Value: 0.1974\n",
                        "  Batch 600/777 | Loss: 0.8694 | Policy: 0.7597 | Value: 0.1097\n",
                        "  Batch 620/777 | Loss: 0.8653 | Policy: 0.5939 | Value: 0.2714\n",
                        "  Batch 640/777 | Loss: 0.8869 | Policy: 0.6921 | Value: 0.1948\n",
                        "  Batch 660/777 | Loss: 0.8398 | Policy: 0.7462 | Value: 0.0936\n",
                        "  Batch 680/777 | Loss: 0.6331 | Policy: 0.5082 | Value: 0.1249\n",
                        "  Batch 700/777 | Loss: 0.9199 | Policy: 0.8184 | Value: 0.1015\n",
                        "  Batch 720/777 | Loss: 1.0875 | Policy: 0.9646 | Value: 0.1230\n",
                        "  Batch 740/777 | Loss: 0.8883 | Policy: 0.7282 | Value: 0.1601\n",
                        "  Batch 760/777 | Loss: 0.6972 | Policy: 0.5356 | Value: 0.1616\n",
                        "\n",
                        "Epoch 2 Average Loss: 0.8551\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/777 | Loss: 0.7421 | Policy: 0.6291 | Value: 0.1129\n",
                        "  Batch  20/777 | Loss: 0.5919 | Policy: 0.4397 | Value: 0.1522\n",
                        "  Batch  40/777 | Loss: 0.5994 | Policy: 0.3930 | Value: 0.2064\n",
                        "  Batch  60/777 | Loss: 0.8661 | Policy: 0.6905 | Value: 0.1757\n",
                        "  Batch  80/777 | Loss: 0.5604 | Policy: 0.4532 | Value: 0.1072\n",
                        "  Batch 100/777 | Loss: 0.4600 | Policy: 0.3448 | Value: 0.1152\n",
                        "  Batch 120/777 | Loss: 0.5907 | Policy: 0.4040 | Value: 0.1866\n",
                        "  Batch 140/777 | Loss: 0.4919 | Policy: 0.4368 | Value: 0.0551\n",
                        "  Batch 160/777 | Loss: 0.6436 | Policy: 0.5389 | Value: 0.1047\n",
                        "  Batch 180/777 | Loss: 0.5753 | Policy: 0.4616 | Value: 0.1137\n",
                        "  Batch 200/777 | Loss: 0.4416 | Policy: 0.3458 | Value: 0.0959\n",
                        "  Batch 220/777 | Loss: 0.6146 | Policy: 0.4364 | Value: 0.1782\n",
                        "  Batch 240/777 | Loss: 0.6346 | Policy: 0.5074 | Value: 0.1273\n",
                        "  Batch 260/777 | Loss: 1.0355 | Policy: 0.8536 | Value: 0.1819\n",
                        "  Batch 280/777 | Loss: 0.5058 | Policy: 0.3664 | Value: 0.1394\n",
                        "  Batch 300/777 | Loss: 0.5002 | Policy: 0.4558 | Value: 0.0444\n",
                        "  Batch 320/777 | Loss: 0.8537 | Policy: 0.6661 | Value: 0.1877\n",
                        "  Batch 340/777 | Loss: 0.5654 | Policy: 0.4639 | Value: 0.1015\n",
                        "  Batch 360/777 | Loss: 0.7267 | Policy: 0.5122 | Value: 0.2145\n",
                        "  Batch 380/777 | Loss: 0.5285 | Policy: 0.4471 | Value: 0.0815\n",
                        "  Batch 400/777 | Loss: 0.5931 | Policy: 0.5436 | Value: 0.0494\n",
                        "  Batch 420/777 | Loss: 0.5042 | Policy: 0.4621 | Value: 0.0421\n",
                        "  Batch 440/777 | Loss: 0.9858 | Policy: 0.7882 | Value: 0.1976\n",
                        "  Batch 460/777 | Loss: 0.8917 | Policy: 0.6965 | Value: 0.1952\n",
                        "  Batch 480/777 | Loss: 1.1860 | Policy: 0.7486 | Value: 0.4374\n",
                        "  Batch 500/777 | Loss: 0.9432 | Policy: 0.5779 | Value: 0.3653\n",
                        "  Batch 520/777 | Loss: 0.9614 | Policy: 0.6414 | Value: 0.3200\n",
                        "  Batch 540/777 | Loss: 0.6723 | Policy: 0.5051 | Value: 0.1672\n",
                        "  Batch 560/777 | Loss: 0.6909 | Policy: 0.5712 | Value: 0.1197\n",
                        "  Batch 580/777 | Loss: 0.4908 | Policy: 0.3897 | Value: 0.1011\n",
                        "  Batch 600/777 | Loss: 0.6028 | Policy: 0.3117 | Value: 0.2911\n",
                        "  Batch 620/777 | Loss: 0.8654 | Policy: 0.6346 | Value: 0.2308\n",
                        "  Batch 640/777 | Loss: 0.7832 | Policy: 0.7022 | Value: 0.0810\n",
                        "  Batch 660/777 | Loss: 1.0503 | Policy: 0.7131 | Value: 0.3372\n",
                        "  Batch 680/777 | Loss: 0.8878 | Policy: 0.8240 | Value: 0.0638\n",
                        "  Batch 700/777 | Loss: 0.9071 | Policy: 0.7640 | Value: 0.1432\n",
                        "  Batch 720/777 | Loss: 0.4863 | Policy: 0.3189 | Value: 0.1674\n",
                        "  Batch 740/777 | Loss: 0.6102 | Policy: 0.4402 | Value: 0.1701\n",
                        "  Batch 760/777 | Loss: 0.6210 | Policy: 0.6031 | Value: 0.0179\n",
                        "\n",
                        "Epoch 3 Average Loss: 0.6763\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: A\n",
                        "Game 2/10 → Winner: A\n",
                        "Game 3/10 → Winner: A\n",
                        "Game 4/10 → Winner: A\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: B\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: B\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 7\n",
                        "Model B wins: 3\n",
                        "Win rate A: 0.70\n",
                        "[main] Arena win rate for new model: 0.70\n",
                        "[main] New model accepted! Copying model_latest -> model_old\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 26 - self-play 32 games\n",
                        "============================================================\n",
                        "[main] Generating 32 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0026.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 26 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 53088 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/829 | Loss: 0.8554 | Policy: 0.6839 | Value: 0.1715\n",
                        "  Batch  20/829 | Loss: 0.5676 | Policy: 0.4217 | Value: 0.1459\n",
                        "  Batch  40/829 | Loss: 0.7072 | Policy: 0.4525 | Value: 0.2547\n",
                        "  Batch  60/829 | Loss: 0.7700 | Policy: 0.6347 | Value: 0.1353\n",
                        "  Batch  80/829 | Loss: 0.7045 | Policy: 0.6155 | Value: 0.0890\n",
                        "  Batch 100/829 | Loss: 0.5986 | Policy: 0.4868 | Value: 0.1118\n",
                        "  Batch 120/829 | Loss: 0.7738 | Policy: 0.6580 | Value: 0.1157\n",
                        "  Batch 140/829 | Loss: 0.7488 | Policy: 0.4208 | Value: 0.3280\n",
                        "  Batch 160/829 | Loss: 0.7781 | Policy: 0.5174 | Value: 0.2607\n",
                        "  Batch 180/829 | Loss: 0.6057 | Policy: 0.5097 | Value: 0.0960\n",
                        "  Batch 200/829 | Loss: 1.0983 | Policy: 0.7199 | Value: 0.3784\n",
                        "  Batch 220/829 | Loss: 0.7423 | Policy: 0.4479 | Value: 0.2944\n",
                        "  Batch 240/829 | Loss: 0.7115 | Policy: 0.5662 | Value: 0.1453\n",
                        "  Batch 260/829 | Loss: 0.9523 | Policy: 0.7132 | Value: 0.2391\n",
                        "  Batch 280/829 | Loss: 0.6503 | Policy: 0.5207 | Value: 0.1296\n",
                        "  Batch 300/829 | Loss: 0.5914 | Policy: 0.3550 | Value: 0.2365\n",
                        "  Batch 320/829 | Loss: 0.5591 | Policy: 0.3735 | Value: 0.1856\n",
                        "  Batch 340/829 | Loss: 1.0093 | Policy: 0.8021 | Value: 0.2072\n",
                        "  Batch 360/829 | Loss: 1.0149 | Policy: 0.8369 | Value: 0.1780\n",
                        "  Batch 380/829 | Loss: 0.6090 | Policy: 0.4098 | Value: 0.1991\n",
                        "  Batch 400/829 | Loss: 0.9927 | Policy: 0.7038 | Value: 0.2889\n",
                        "  Batch 420/829 | Loss: 1.0400 | Policy: 0.7166 | Value: 0.3234\n",
                        "  Batch 440/829 | Loss: 0.6517 | Policy: 0.4888 | Value: 0.1628\n",
                        "  Batch 460/829 | Loss: 0.5438 | Policy: 0.4467 | Value: 0.0971\n",
                        "  Batch 480/829 | Loss: 0.5544 | Policy: 0.3954 | Value: 0.1591\n",
                        "  Batch 500/829 | Loss: 1.1848 | Policy: 0.9761 | Value: 0.2087\n",
                        "  Batch 520/829 | Loss: 0.5555 | Policy: 0.4363 | Value: 0.1192\n",
                        "  Batch 540/829 | Loss: 0.9778 | Policy: 0.6739 | Value: 0.3039\n",
                        "  Batch 560/829 | Loss: 0.7131 | Policy: 0.5302 | Value: 0.1829\n",
                        "  Batch 580/829 | Loss: 1.0418 | Policy: 0.8012 | Value: 0.2406\n",
                        "  Batch 600/829 | Loss: 0.7138 | Policy: 0.5345 | Value: 0.1793\n",
                        "  Batch 620/829 | Loss: 0.8261 | Policy: 0.5823 | Value: 0.2438\n",
                        "  Batch 640/829 | Loss: 0.9560 | Policy: 0.6036 | Value: 0.3524\n",
                        "  Batch 660/829 | Loss: 0.6217 | Policy: 0.4612 | Value: 0.1606\n",
                        "  Batch 680/829 | Loss: 0.5049 | Policy: 0.4192 | Value: 0.0857\n",
                        "  Batch 700/829 | Loss: 1.0916 | Policy: 0.7493 | Value: 0.3423\n",
                        "  Batch 720/829 | Loss: 0.8099 | Policy: 0.5830 | Value: 0.2269\n",
                        "  Batch 740/829 | Loss: 0.8811 | Policy: 0.6525 | Value: 0.2286\n",
                        "  Batch 760/829 | Loss: 1.0163 | Policy: 0.7609 | Value: 0.2554\n",
                        "  Batch 780/829 | Loss: 0.8265 | Policy: 0.7235 | Value: 0.1030\n",
                        "  Batch 800/829 | Loss: 0.9025 | Policy: 0.6946 | Value: 0.2079\n",
                        "  Batch 820/829 | Loss: 0.6916 | Policy: 0.5392 | Value: 0.1525\n",
                        "\n",
                        "Epoch 1 Average Loss: 0.7861\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/829 | Loss: 0.4364 | Policy: 0.3828 | Value: 0.0536\n",
                        "  Batch  20/829 | Loss: 0.5128 | Policy: 0.4001 | Value: 0.1128\n",
                        "  Batch  40/829 | Loss: 0.5765 | Policy: 0.4456 | Value: 0.1309\n",
                        "  Batch  60/829 | Loss: 0.4734 | Policy: 0.3600 | Value: 0.1134\n",
                        "  Batch  80/829 | Loss: 0.6568 | Policy: 0.5105 | Value: 0.1464\n",
                        "  Batch 100/829 | Loss: 0.5108 | Policy: 0.4184 | Value: 0.0923\n",
                        "  Batch 120/829 | Loss: 0.4035 | Policy: 0.2463 | Value: 0.1573\n",
                        "  Batch 140/829 | Loss: 0.2645 | Policy: 0.1797 | Value: 0.0848\n",
                        "  Batch 160/829 | Loss: 0.4154 | Policy: 0.3390 | Value: 0.0763\n",
                        "  Batch 180/829 | Loss: 0.7444 | Policy: 0.4739 | Value: 0.2705\n",
                        "  Batch 200/829 | Loss: 0.6918 | Policy: 0.5735 | Value: 0.1183\n",
                        "  Batch 220/829 | Loss: 0.4183 | Policy: 0.3325 | Value: 0.0859\n",
                        "  Batch 240/829 | Loss: 0.5634 | Policy: 0.4907 | Value: 0.0727\n",
                        "  Batch 260/829 | Loss: 0.2706 | Policy: 0.2458 | Value: 0.0248\n",
                        "  Batch 280/829 | Loss: 0.5244 | Policy: 0.3803 | Value: 0.1442\n",
                        "  Batch 300/829 | Loss: 0.6464 | Policy: 0.5280 | Value: 0.1184\n",
                        "  Batch 320/829 | Loss: 0.5373 | Policy: 0.3299 | Value: 0.2074\n",
                        "  Batch 340/829 | Loss: 0.4742 | Policy: 0.2523 | Value: 0.2219\n",
                        "  Batch 360/829 | Loss: 0.4339 | Policy: 0.2937 | Value: 0.1402\n",
                        "  Batch 380/829 | Loss: 0.6957 | Policy: 0.4768 | Value: 0.2189\n",
                        "  Batch 400/829 | Loss: 0.6127 | Policy: 0.4376 | Value: 0.1751\n",
                        "  Batch 420/829 | Loss: 0.5808 | Policy: 0.3548 | Value: 0.2260\n",
                        "  Batch 440/829 | Loss: 0.7652 | Policy: 0.7068 | Value: 0.0584\n",
                        "  Batch 460/829 | Loss: 0.5206 | Policy: 0.3243 | Value: 0.1962\n",
                        "  Batch 480/829 | Loss: 0.3735 | Policy: 0.2902 | Value: 0.0833\n",
                        "  Batch 500/829 | Loss: 0.5603 | Policy: 0.4253 | Value: 0.1350\n",
                        "  Batch 520/829 | Loss: 0.6670 | Policy: 0.5159 | Value: 0.1510\n",
                        "  Batch 540/829 | Loss: 0.7718 | Policy: 0.5519 | Value: 0.2199\n",
                        "  Batch 560/829 | Loss: 0.8479 | Policy: 0.6494 | Value: 0.1985\n",
                        "  Batch 580/829 | Loss: 0.5313 | Policy: 0.3638 | Value: 0.1675\n",
                        "  Batch 600/829 | Loss: 0.7177 | Policy: 0.5958 | Value: 0.1219\n",
                        "  Batch 620/829 | Loss: 0.6414 | Policy: 0.5290 | Value: 0.1123\n",
                        "  Batch 640/829 | Loss: 0.6641 | Policy: 0.6071 | Value: 0.0571\n",
                        "  Batch 660/829 | Loss: 0.7413 | Policy: 0.4879 | Value: 0.2534\n",
                        "  Batch 680/829 | Loss: 0.4909 | Policy: 0.3332 | Value: 0.1576\n",
                        "  Batch 700/829 | Loss: 0.3798 | Policy: 0.3185 | Value: 0.0613\n",
                        "  Batch 720/829 | Loss: 0.7291 | Policy: 0.5441 | Value: 0.1851\n",
                        "  Batch 740/829 | Loss: 0.6157 | Policy: 0.5051 | Value: 0.1106\n",
                        "  Batch 760/829 | Loss: 0.5298 | Policy: 0.3125 | Value: 0.2173\n",
                        "  Batch 780/829 | Loss: 0.8851 | Policy: 0.7571 | Value: 0.1280\n",
                        "  Batch 800/829 | Loss: 0.7528 | Policy: 0.5676 | Value: 0.1851\n",
                        "  Batch 820/829 | Loss: 0.7239 | Policy: 0.6297 | Value: 0.0942\n",
                        "\n",
                        "Epoch 2 Average Loss: 0.6191\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/829 | Loss: 0.5728 | Policy: 0.4547 | Value: 0.1182\n",
                        "  Batch  20/829 | Loss: 0.4764 | Policy: 0.2966 | Value: 0.1798\n",
                        "  Batch  40/829 | Loss: 0.3269 | Policy: 0.2475 | Value: 0.0794\n",
                        "  Batch  60/829 | Loss: 0.4427 | Policy: 0.3658 | Value: 0.0769\n",
                        "  Batch  80/829 | Loss: 0.3907 | Policy: 0.2923 | Value: 0.0983\n",
                        "  Batch 100/829 | Loss: 0.4206 | Policy: 0.3119 | Value: 0.1087\n",
                        "  Batch 120/829 | Loss: 0.2707 | Policy: 0.2012 | Value: 0.0695\n",
                        "  Batch 140/829 | Loss: 0.6800 | Policy: 0.5504 | Value: 0.1296\n",
                        "  Batch 160/829 | Loss: 0.3484 | Policy: 0.3215 | Value: 0.0269\n",
                        "  Batch 180/829 | Loss: 0.5941 | Policy: 0.2968 | Value: 0.2972\n",
                        "  Batch 200/829 | Loss: 0.4367 | Policy: 0.3334 | Value: 0.1033\n",
                        "  Batch 220/829 | Loss: 0.4448 | Policy: 0.3383 | Value: 0.1065\n",
                        "  Batch 240/829 | Loss: 0.5391 | Policy: 0.4654 | Value: 0.0737\n",
                        "  Batch 260/829 | Loss: 0.7776 | Policy: 0.5364 | Value: 0.2412\n",
                        "  Batch 280/829 | Loss: 0.5358 | Policy: 0.3917 | Value: 0.1441\n",
                        "  Batch 300/829 | Loss: 0.2335 | Policy: 0.2290 | Value: 0.0044\n",
                        "  Batch 320/829 | Loss: 0.4538 | Policy: 0.3595 | Value: 0.0943\n",
                        "  Batch 340/829 | Loss: 0.3583 | Policy: 0.2622 | Value: 0.0962\n",
                        "  Batch 360/829 | Loss: 0.5902 | Policy: 0.3779 | Value: 0.2124\n",
                        "  Batch 380/829 | Loss: 0.3928 | Policy: 0.3265 | Value: 0.0663\n",
                        "  Batch 400/829 | Loss: 0.5195 | Policy: 0.3846 | Value: 0.1349\n",
                        "  Batch 420/829 | Loss: 0.5870 | Policy: 0.4339 | Value: 0.1532\n",
                        "  Batch 440/829 | Loss: 0.4716 | Policy: 0.3444 | Value: 0.1271\n",
                        "  Batch 460/829 | Loss: 0.6060 | Policy: 0.4608 | Value: 0.1452\n",
                        "  Batch 480/829 | Loss: 0.3690 | Policy: 0.2517 | Value: 0.1173\n",
                        "  Batch 500/829 | Loss: 0.8083 | Policy: 0.5804 | Value: 0.2279\n",
                        "  Batch 520/829 | Loss: 0.6807 | Policy: 0.5137 | Value: 0.1669\n",
                        "  Batch 540/829 | Loss: 0.3902 | Policy: 0.3628 | Value: 0.0273\n",
                        "  Batch 560/829 | Loss: 0.4862 | Policy: 0.3590 | Value: 0.1272\n",
                        "  Batch 580/829 | Loss: 0.5473 | Policy: 0.4370 | Value: 0.1102\n",
                        "  Batch 600/829 | Loss: 0.4217 | Policy: 0.3197 | Value: 0.1020\n",
                        "  Batch 620/829 | Loss: 0.6874 | Policy: 0.4793 | Value: 0.2081\n",
                        "  Batch 640/829 | Loss: 0.4173 | Policy: 0.3158 | Value: 0.1015\n",
                        "  Batch 660/829 | Loss: 0.5617 | Policy: 0.4245 | Value: 0.1371\n",
                        "  Batch 680/829 | Loss: 0.4729 | Policy: 0.2839 | Value: 0.1890\n",
                        "  Batch 700/829 | Loss: 0.7538 | Policy: 0.5763 | Value: 0.1775\n",
                        "  Batch 720/829 | Loss: 0.6138 | Policy: 0.5249 | Value: 0.0889\n",
                        "  Batch 740/829 | Loss: 0.5493 | Policy: 0.4431 | Value: 0.1063\n",
                        "  Batch 760/829 | Loss: 0.5400 | Policy: 0.4161 | Value: 0.1239\n",
                        "  Batch 780/829 | Loss: 0.7880 | Policy: 0.4899 | Value: 0.2981\n",
                        "  Batch 800/829 | Loss: 0.7856 | Policy: 0.6302 | Value: 0.1554\n",
                        "  Batch 820/829 | Loss: 0.3687 | Policy: 0.2798 | Value: 0.0889\n",
                        "\n",
                        "Epoch 3 Average Loss: 0.5234\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: B\n",
                        "Game 2/10 → Winner: A\n",
                        "Game 3/10 → Winner: B\n",
                        "Game 4/10 → Winner: A\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: B\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: A\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 6\n",
                        "Model B wins: 4\n",
                        "Win rate A: 0.60\n",
                        "[main] Arena win rate for new model: 0.60\n",
                        "[main] New model accepted! Copying model_latest -> model_old\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 27 - self-play 32 games\n",
                        "============================================================\n",
                        "[main] Generating 32 games concurrently...\n"
                    ]
                }
            ],
            "source": [
                "from main import main_loop\n",
                "\n",
                "await main_loop(max_iterations=50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download_header"
            },
            "source": [
                "## 5. Download Trained Model\n",
                "Download the latest model checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_model"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "from pathlib import Path\n",
                "\n",
                "model_path = Path(\"data/checkpoints/model_latest.pth\")\n",
                "if model_path.exists():\n",
                "    files.download(str(model_path))\n",
                "    print(f\"Downloaded: {model_path}\")\n",
                "else:\n",
                "    print(\"No model found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "monitoring_header"
            },
            "source": [
                "## 6. Monitoring (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training_status"
            },
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "CHECKPOINT_DIR = Path(\"data/checkpoints\")\n",
                "REPLAY_DIR = Path(\"data/replay\")\n",
                "\n",
                "def show_training_status():\n",
                "    print(\"=\" * 50)\n",
                "    print(\"TRAINING STATUS\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    if CHECKPOINT_DIR.exists():\n",
                "        checkpoints = list(CHECKPOINT_DIR.glob(\"*.pth\"))\n",
                "        print(f\"\\nCheckpoints: {len(checkpoints)}\")\n",
                "        for cp in checkpoints:\n",
                "            size_mb = cp.stat().st_size / (1024 * 1024)\n",
                "            print(f\"  - {cp.name}: {size_mb:.2f} MB\")\n",
                "    \n",
                "    if REPLAY_DIR.exists():\n",
                "        replays = list(REPLAY_DIR.glob(\"*.npz\"))\n",
                "        print(f\"\\nReplay batches: {len(replays)}\")\n",
                "        if replays:\n",
                "            total_size = sum(r.stat().st_size for r in replays) / (1024 * 1024)\n",
                "            print(f\"  Total size: {total_size:.2f} MB\")\n",
                "    \n",
                "    if torch.cuda.is_available():\n",
                "        print(\"\\nGPU Memory:\")\n",
                "        print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
                "        print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
                "\n",
                "show_training_status()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
