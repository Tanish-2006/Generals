{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# GENERAL: Strategic Game AI Training\n",
                "\n",
                "GPU-accelerated training using AlphaZero-style RL. Optimizations enabled for Colab (T4) and Local (RTX) environments.\n",
                "\n",
                "This notebook uses the optimized `GenGameAI` codebase with:\n",
                "- **Non-blocking Inference** (Asyncio + ThreadPool)\n",
                "- **Efficient Data Loading** (DataLoader with pinned memory)\n",
                "- **Auto-Configuration** (Detects GPU VRAM and CPU cores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "!git clone https://github.com/Tanish-2006/Generals.git\n",
                "%cd Generals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "!python3.11 -m pip install torch numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "e:\\Projects\\GenGameAI\n"
                    ]
                }
            ],
            "source": [
                "%cd .."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config_header"
            },
            "source": [
                "## 2. Configuration & Hardware Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify_config"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
                        "VRAM: 8.59 GB\n",
                        "\n",
                        "Auto-Detected Configuration:\n",
                        "  Workers: 0\n",
                        "  Batch Size: 64\n",
                        "  Games/Iter: 16\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "import torch\n",
                "from config import TRAINING\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected.\")\n",
                "\n",
                "print(\"\\nAuto-Detected Configuration:\")\n",
                "print(f\"  Workers: {TRAINING.num_workers}\")\n",
                "print(f\"  Batch Size: {TRAINING.batch_size}\")\n",
                "print(f\"  Games/Iter: {TRAINING.games_per_iter}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "upload_header"
            },
            "source": [
                "## 3. Upload Previous Model (Optional)\n",
                "If you have a `model_latest.pth` or `model_old.pth` from a previous run, upload it here to resume training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "upload_model"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "from pathlib import Path\n",
                "\n",
                "CHECKPOINT_DIR = Path(\"data/checkpoints\")\n",
                "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"Upload model_latest.pth or model_old.pth if you have one:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    target_path = CHECKPOINT_DIR / filename\n",
                "    with open(target_path, 'wb') as f:\n",
                "        f.write(uploaded[filename])\n",
                "    print(f\"Saved {filename} to {target_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training_header"
            },
            "source": [
                "## 4. Training Loop\n",
                "Runs the main optimized training loop. Supports resuming if models exist."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "run_training"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[main] Found 1 replay batches. Resuming from iteration 2.\n",
                        "[Trainer] Using device: cuda\n",
                        "[Trainer] AMP enabled for faster training\n",
                        "[Trainer] JIT compilation skipped on Windows (Triton not supported)\n",
                        "[main] Resuming with Best Model (model_old.pth)\n",
                        "[main] Successfully loaded model from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_old.pth\n",
                        "[InferenceServer] Started on cuda:0 with batch_size=16\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 2 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0002.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 2 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 3560 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/55 | Loss: 9.9759 | Policy: 8.9100 | Value: 1.0659\n",
                        "  Batch  20/55 | Loss: 8.9699 | Policy: 8.3899 | Value: 0.5799\n",
                        "  Batch  40/55 | Loss: 8.6113 | Policy: 8.0985 | Value: 0.5128\n",
                        "\n",
                        "Epoch 1 Average Loss: 9.0051\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/55 | Loss: 7.6151 | Policy: 7.2907 | Value: 0.3244\n",
                        "  Batch  20/55 | Loss: 7.5845 | Policy: 7.4293 | Value: 0.1552\n",
                        "  Batch  40/55 | Loss: 7.0510 | Policy: 6.8912 | Value: 0.1599\n",
                        "\n",
                        "Epoch 2 Average Loss: 7.3963\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/55 | Loss: 6.8447 | Policy: 6.7000 | Value: 0.1447\n",
                        "  Batch  20/55 | Loss: 6.6273 | Policy: 6.3803 | Value: 0.2470\n",
                        "  Batch  40/55 | Loss: 6.6025 | Policy: 6.5529 | Value: 0.0495\n",
                        "\n",
                        "Epoch 3 Average Loss: 6.7514\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: A\n",
                        "Game 2/10 → Winner: A\n",
                        "Game 3/10 → Winner: A\n",
                        "Game 4/10 → Winner: B\n",
                        "Game 5/10 → Winner: A\n",
                        "Game 6/10 → Winner: A\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: A\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 9\n",
                        "Model B wins: 1\n",
                        "Win rate A: 0.90\n",
                        "[main] Arena win rate for new model: 0.90\n",
                        "[main] New model accepted! Copying model_latest -> model_old\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 3 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0003.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 3 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 5505 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/86 | Loss: 6.8869 | Policy: 6.4476 | Value: 0.4394\n",
                        "  Batch  20/86 | Loss: 6.9015 | Policy: 6.5837 | Value: 0.3178\n",
                        "  Batch  40/86 | Loss: 6.7993 | Policy: 6.5909 | Value: 0.2084\n",
                        "  Batch  60/86 | Loss: 6.9259 | Policy: 6.5608 | Value: 0.3652\n",
                        "  Batch  80/86 | Loss: 7.0742 | Policy: 6.6703 | Value: 0.4039\n",
                        "\n",
                        "Epoch 1 Average Loss: 6.7479\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/86 | Loss: 5.9133 | Policy: 5.7117 | Value: 0.2015\n",
                        "  Batch  20/86 | Loss: 5.5725 | Policy: 5.3975 | Value: 0.1750\n",
                        "  Batch  40/86 | Loss: 5.8993 | Policy: 5.6821 | Value: 0.2172\n",
                        "  Batch  60/86 | Loss: 5.1445 | Policy: 4.7260 | Value: 0.4185\n",
                        "  Batch  80/86 | Loss: 5.1313 | Policy: 5.0189 | Value: 0.1124\n",
                        "\n",
                        "Epoch 2 Average Loss: 5.5716\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/86 | Loss: 4.1430 | Policy: 4.1229 | Value: 0.0200\n",
                        "  Batch  20/86 | Loss: 3.3611 | Policy: 3.2009 | Value: 0.1602\n",
                        "  Batch  40/86 | Loss: 3.3917 | Policy: 3.3697 | Value: 0.0220\n",
                        "  Batch  60/86 | Loss: 3.3164 | Policy: 3.2207 | Value: 0.0957\n",
                        "  Batch  80/86 | Loss: 3.4891 | Policy: 3.1506 | Value: 0.3385\n",
                        "\n",
                        "Epoch 3 Average Loss: 3.4738\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: B\n",
                        "Game 2/10 → Winner: A\n",
                        "Game 3/10 → Winner: A\n",
                        "Game 4/10 → Winner: B\n",
                        "Game 5/10 → Winner: A\n",
                        "Game 6/10 → Winner: B\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: B\n",
                        "Game 9/10 → Winner: A\n",
                        "Game 10/10 → Winner: B\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 5\n",
                        "Model B wins: 5\n",
                        "Win rate A: 0.50\n",
                        "[main] Arena win rate for new model: 0.50\n",
                        "[main] New model rejected. Keeping previous model_old.\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 4 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0004.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 4 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 7592 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/118 | Loss: 3.5895 | Policy: 3.1458 | Value: 0.4437\n",
                        "  Batch  20/118 | Loss: 2.6175 | Policy: 2.3569 | Value: 0.2606\n",
                        "  Batch  40/118 | Loss: 3.0108 | Policy: 2.7987 | Value: 0.2121\n",
                        "  Batch  60/118 | Loss: 3.1657 | Policy: 2.8807 | Value: 0.2850\n",
                        "  Batch  80/118 | Loss: 3.1410 | Policy: 2.8066 | Value: 0.3343\n",
                        "  Batch 100/118 | Loss: 2.6165 | Policy: 2.3199 | Value: 0.2966\n",
                        "\n",
                        "Epoch 1 Average Loss: 3.0449\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/118 | Loss: 1.4273 | Policy: 1.2574 | Value: 0.1698\n",
                        "  Batch  20/118 | Loss: 1.5684 | Policy: 1.4162 | Value: 0.1522\n",
                        "  Batch  40/118 | Loss: 1.3828 | Policy: 1.2729 | Value: 0.1099\n",
                        "  Batch  60/118 | Loss: 1.6222 | Policy: 1.4981 | Value: 0.1240\n",
                        "  Batch  80/118 | Loss: 2.1003 | Policy: 1.8739 | Value: 0.2265\n",
                        "  Batch 100/118 | Loss: 1.3058 | Policy: 1.1912 | Value: 0.1146\n",
                        "\n",
                        "Epoch 2 Average Loss: 1.8382\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/118 | Loss: 1.1842 | Policy: 0.9396 | Value: 0.2446\n",
                        "  Batch  20/118 | Loss: 1.1741 | Policy: 0.8808 | Value: 0.2933\n",
                        "  Batch  40/118 | Loss: 1.4398 | Policy: 1.2993 | Value: 0.1405\n",
                        "  Batch  60/118 | Loss: 1.0197 | Policy: 0.9535 | Value: 0.0663\n",
                        "  Batch  80/118 | Loss: 1.3400 | Policy: 1.1313 | Value: 0.2087\n",
                        "  Batch 100/118 | Loss: 1.2959 | Policy: 1.1283 | Value: 0.1676\n",
                        "\n",
                        "Epoch 3 Average Loss: 1.1504\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: A\n",
                        "Game 2/10 → Winner: A\n",
                        "Game 3/10 → Winner: A\n",
                        "Game 4/10 → Winner: A\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: B\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: B\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 7\n",
                        "Model B wins: 3\n",
                        "Win rate A: 0.70\n",
                        "[main] Arena win rate for new model: 0.70\n",
                        "[main] New model accepted! Copying model_latest -> model_old\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 5 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0005.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 5 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 9472 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/148 | Loss: 1.2286 | Policy: 1.0670 | Value: 0.1617\n",
                        "  Batch  20/148 | Loss: 1.6026 | Policy: 1.4412 | Value: 0.1614\n",
                        "  Batch  40/148 | Loss: 1.6626 | Policy: 1.4993 | Value: 0.1634\n",
                        "  Batch  60/148 | Loss: 1.4874 | Policy: 1.4037 | Value: 0.0837\n",
                        "  Batch  80/148 | Loss: 1.3436 | Policy: 1.2126 | Value: 0.1310\n",
                        "  Batch 100/148 | Loss: 1.3974 | Policy: 1.1322 | Value: 0.2652\n",
                        "  Batch 120/148 | Loss: 1.7626 | Policy: 1.5925 | Value: 0.1701\n",
                        "  Batch 140/148 | Loss: 1.9409 | Policy: 1.5407 | Value: 0.4002\n",
                        "\n",
                        "Epoch 1 Average Loss: 1.6081\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/148 | Loss: 1.0229 | Policy: 0.8937 | Value: 0.1292\n",
                        "  Batch  20/148 | Loss: 1.1109 | Policy: 0.8794 | Value: 0.2315\n",
                        "  Batch  40/148 | Loss: 1.0025 | Policy: 0.8637 | Value: 0.1389\n",
                        "  Batch  60/148 | Loss: 1.0537 | Policy: 0.8582 | Value: 0.1954\n",
                        "  Batch  80/148 | Loss: 0.8035 | Policy: 0.7546 | Value: 0.0489\n",
                        "  Batch 100/148 | Loss: 0.8822 | Policy: 0.7209 | Value: 0.1613\n",
                        "  Batch 120/148 | Loss: 1.1061 | Policy: 0.9527 | Value: 0.1534\n",
                        "  Batch 140/148 | Loss: 0.7848 | Policy: 0.7083 | Value: 0.0765\n",
                        "\n",
                        "Epoch 2 Average Loss: 0.9549\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/148 | Loss: 0.4791 | Policy: 0.4616 | Value: 0.0175\n",
                        "  Batch  20/148 | Loss: 0.6452 | Policy: 0.4190 | Value: 0.2261\n",
                        "  Batch  40/148 | Loss: 0.7430 | Policy: 0.6507 | Value: 0.0923\n",
                        "  Batch  60/148 | Loss: 0.7819 | Policy: 0.6671 | Value: 0.1148\n",
                        "  Batch  80/148 | Loss: 0.6800 | Policy: 0.6331 | Value: 0.0469\n",
                        "  Batch 100/148 | Loss: 0.5758 | Policy: 0.5573 | Value: 0.0185\n",
                        "  Batch 120/148 | Loss: 0.7436 | Policy: 0.6358 | Value: 0.1078\n",
                        "  Batch 140/148 | Loss: 0.6329 | Policy: 0.5062 | Value: 0.1267\n",
                        "\n",
                        "Epoch 3 Average Loss: 0.6388\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: B\n",
                        "Game 2/10 → Winner: B\n",
                        "Game 3/10 → Winner: B\n",
                        "Game 4/10 → Winner: B\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: B\n",
                        "Game 7/10 → Winner: B\n",
                        "Game 8/10 → Winner: B\n",
                        "Game 9/10 → Winner: B\n",
                        "Game 10/10 → Winner: A\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 1\n",
                        "Model B wins: 9\n",
                        "Win rate A: 0.10\n",
                        "[main] Arena win rate for new model: 0.10\n",
                        "[main] New model rejected. Keeping previous model_old.\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 6 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n",
                        "[ReplayBuffer] Saved: E:\\Projects\\GenGameAI\\data\\replay\\batch_0006.npz\n",
                        "[main] Loading replay data to train\n",
                        "[ReplayBuffer] Loaded 6 batches\n",
                        "[main] Training for 3 epochs...\n",
                        "\n",
                        "[Trainer] Training on 10700 samples\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/3\n",
                        "============================================================\n",
                        "  Batch   0/167 | Loss: 1.0965 | Policy: 0.8049 | Value: 0.2916\n",
                        "  Batch  20/167 | Loss: 1.0853 | Policy: 0.9598 | Value: 0.1255\n",
                        "  Batch  40/167 | Loss: 1.0985 | Policy: 0.9808 | Value: 0.1176\n",
                        "  Batch  60/167 | Loss: 1.1718 | Policy: 0.9340 | Value: 0.2378\n",
                        "  Batch  80/167 | Loss: 1.1914 | Policy: 0.9195 | Value: 0.2720\n",
                        "  Batch 100/167 | Loss: 1.1598 | Policy: 0.9957 | Value: 0.1641\n",
                        "  Batch 120/167 | Loss: 0.7272 | Policy: 0.5739 | Value: 0.1533\n",
                        "  Batch 140/167 | Loss: 1.2540 | Policy: 1.1117 | Value: 0.1423\n",
                        "  Batch 160/167 | Loss: 1.5441 | Policy: 1.3006 | Value: 0.2434\n",
                        "\n",
                        "Epoch 1 Average Loss: 1.0067\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 2/3\n",
                        "============================================================\n",
                        "  Batch   0/167 | Loss: 0.6001 | Policy: 0.4638 | Value: 0.1363\n",
                        "  Batch  20/167 | Loss: 0.2990 | Policy: 0.2616 | Value: 0.0374\n",
                        "  Batch  40/167 | Loss: 0.5345 | Policy: 0.4726 | Value: 0.0620\n",
                        "  Batch  60/167 | Loss: 0.5141 | Policy: 0.4797 | Value: 0.0345\n",
                        "  Batch  80/167 | Loss: 0.6976 | Policy: 0.5754 | Value: 0.1222\n",
                        "  Batch 100/167 | Loss: 0.6560 | Policy: 0.5304 | Value: 0.1257\n",
                        "  Batch 120/167 | Loss: 0.4330 | Policy: 0.4279 | Value: 0.0051\n",
                        "  Batch 140/167 | Loss: 0.7972 | Policy: 0.6535 | Value: 0.1437\n",
                        "  Batch 160/167 | Loss: 0.6991 | Policy: 0.6409 | Value: 0.0582\n",
                        "\n",
                        "Epoch 2 Average Loss: 0.5648\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 3/3\n",
                        "============================================================\n",
                        "  Batch   0/167 | Loss: 0.3715 | Policy: 0.3235 | Value: 0.0480\n",
                        "  Batch  20/167 | Loss: 0.3426 | Policy: 0.3010 | Value: 0.0416\n",
                        "  Batch  40/167 | Loss: 0.3346 | Policy: 0.2842 | Value: 0.0504\n",
                        "  Batch  60/167 | Loss: 0.4371 | Policy: 0.3523 | Value: 0.0848\n",
                        "  Batch  80/167 | Loss: 0.3431 | Policy: 0.2724 | Value: 0.0707\n",
                        "  Batch 100/167 | Loss: 0.3520 | Policy: 0.2974 | Value: 0.0546\n",
                        "  Batch 120/167 | Loss: 0.4413 | Policy: 0.4044 | Value: 0.0370\n",
                        "  Batch 140/167 | Loss: 0.4602 | Policy: 0.3186 | Value: 0.1416\n",
                        "  Batch 160/167 | Loss: 0.5207 | Policy: 0.3584 | Value: 0.1622\n",
                        "\n",
                        "Epoch 3 Average Loss: 0.4433\n",
                        "\n",
                        "[Trainer] Model saved to: E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Reloading InferenceServer with new model weights...\n",
                        "[InferenceServer] Reloaded weights from E:\\Projects\\GenGameAI\\data\\checkpoints\\model_latest.pth\n",
                        "[main] Evaluating new model vs previous model (Arena)\n",
                        "[Arena] Using device: cuda\n",
                        "Game 1/10 → Winner: B\n",
                        "Game 2/10 → Winner: B\n",
                        "Game 3/10 → Winner: B\n",
                        "Game 4/10 → Winner: A\n",
                        "Game 5/10 → Winner: B\n",
                        "Game 6/10 → Winner: A\n",
                        "Game 7/10 → Winner: A\n",
                        "Game 8/10 → Winner: A\n",
                        "Game 9/10 → Winner: B\n",
                        "Game 10/10 → Winner: B\n",
                        "\n",
                        "=== Arena Results ===\n",
                        "Model A wins: 4\n",
                        "Model B wins: 6\n",
                        "Win rate A: 0.40\n",
                        "[main] Arena win rate for new model: 0.40\n",
                        "[main] New model rejected. Keeping previous model_old.\n",
                        "\n",
                        "============================================================\n",
                        "[main] ITERATION 7 - self-play 16 games\n",
                        "============================================================\n",
                        "[main] Generating 16 games concurrently...\n"
                    ]
                },
                {
                    "ename": "CancelledError",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main_loop\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main_loop(max_iterations=\u001b[32m20\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\main.py:124\u001b[39m, in \u001b[36mmain_loop\u001b[39m\u001b[34m(max_iterations)\u001b[39m\n\u001b[32m    115\u001b[39m sp = SelfPlay(\n\u001b[32m    116\u001b[39m     GeneralsEnv,\n\u001b[32m    117\u001b[39m     inference_server,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     temperature_threshold=TRAINING.temperature_threshold,\n\u001b[32m    121\u001b[39m )\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[main] Generating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAINING.games_per_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m games concurrently...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m states, policies, values = \u001b[38;5;28;01mawait\u001b[39;00m sp.play_iteration()\n\u001b[32m    126\u001b[39m replay.add_game(states, policies, values)\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[main] Loading replay data to train\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\selfplay\\selfplay.py:23\u001b[39m, in \u001b[36mSelfPlay.play_iteration\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplay_iteration\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     22\u001b[39m     tasks = [\u001b[38;5;28mself\u001b[39m.play_one_game() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.games_per_iteration)]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     25\u001b[39m     all_states = []\n\u001b[32m     26\u001b[39m     all_policies = []\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\selfplay\\selfplay.py:48\u001b[39m, in \u001b[36mSelfPlay.play_one_game\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     47\u001b[39m     mcts = AsyncMCTS(env, inference_server=\u001b[38;5;28mself\u001b[39m.inference_server, c_puct=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m mcts.search(n_sims=\u001b[38;5;28mself\u001b[39m.mcts_simulations)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m move_counter < \u001b[38;5;28mself\u001b[39m.temperature_threshold:\n\u001b[32m     51\u001b[39m         temp = \u001b[32m1.0\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\mcts\\mcts.py:40\u001b[39m, in \u001b[36mAsyncMCTS.search\u001b[39m\u001b[34m(self, n_sims)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_sims):\n\u001b[32m     39\u001b[39m     saved_state = \u001b[38;5;28mself\u001b[39m.root_env.save_state()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._simulate(\u001b[38;5;28mself\u001b[39m.root_env, \u001b[38;5;28mself\u001b[39m.root)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_env.restore_state(saved_state)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\mcts\\mcts.py:166\u001b[39m, in \u001b[36mAsyncMCTS._simulate\u001b[39m\u001b[34m(self, env, node)\u001b[39m\n\u001b[32m    163\u001b[39m     best_child.update(leaf_value)\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf_value\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m leaf_value = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._simulate(env, best_child)\n\u001b[32m    167\u001b[39m leaf_value = -leaf_value\n\u001b[32m    168\u001b[39m best_child.update(leaf_value)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\mcts\\mcts.py:166\u001b[39m, in \u001b[36mAsyncMCTS._simulate\u001b[39m\u001b[34m(self, env, node)\u001b[39m\n\u001b[32m    163\u001b[39m     best_child.update(leaf_value)\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf_value\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m leaf_value = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._simulate(env, best_child)\n\u001b[32m    167\u001b[39m leaf_value = -leaf_value\n\u001b[32m    168\u001b[39m best_child.update(leaf_value)\n",
                        "    \u001b[31m[... skipping similar frames: AsyncMCTS._simulate at line 166 (9 times)]\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\mcts\\mcts.py:166\u001b[39m, in \u001b[36mAsyncMCTS._simulate\u001b[39m\u001b[34m(self, env, node)\u001b[39m\n\u001b[32m    163\u001b[39m     best_child.update(leaf_value)\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf_value\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m leaf_value = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._simulate(env, best_child)\n\u001b[32m    167\u001b[39m leaf_value = -leaf_value\n\u001b[32m    168\u001b[39m best_child.update(leaf_value)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\mcts\\mcts.py:104\u001b[39m, in \u001b[36mAsyncMCTS._simulate\u001b[39m\u001b[34m(self, env, node)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_server \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     state_tensor = env.encode_state(env.dice_value)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     policy_logits, value = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_server.predict(state_tensor)\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(policy_logits, np.ndarray):\n\u001b[32m    107\u001b[39m         logits = policy_logits\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\GenGameAI\\utils\\batched_inference.py:43\u001b[39m, in \u001b[36mInferenceServer.predict\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     41\u001b[39m future = asyncio.Future()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.queue.put((state, future))\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
                        "\u001b[31mCancelledError\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "from main import main_loop\n",
                "\n",
                "await main_loop(max_iterations=20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download_header"
            },
            "source": [
                "## 5. Download Trained Model\n",
                "Download the latest model checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_model"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "from pathlib import Path\n",
                "\n",
                "model_path = Path(\"data/checkpoints/model_latest.pth\")\n",
                "if model_path.exists():\n",
                "    files.download(str(model_path))\n",
                "    print(f\"Downloaded: {model_path}\")\n",
                "else:\n",
                "    print(\"No model found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "monitoring_header"
            },
            "source": [
                "## 6. Monitoring (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training_status"
            },
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "CHECKPOINT_DIR = Path(\"data/checkpoints\")\n",
                "REPLAY_DIR = Path(\"data/replay\")\n",
                "\n",
                "def show_training_status():\n",
                "    print(\"=\" * 50)\n",
                "    print(\"TRAINING STATUS\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    if CHECKPOINT_DIR.exists():\n",
                "        checkpoints = list(CHECKPOINT_DIR.glob(\"*.pth\"))\n",
                "        print(f\"\\nCheckpoints: {len(checkpoints)}\")\n",
                "        for cp in checkpoints:\n",
                "            size_mb = cp.stat().st_size / (1024 * 1024)\n",
                "            print(f\"  - {cp.name}: {size_mb:.2f} MB\")\n",
                "    \n",
                "    if REPLAY_DIR.exists():\n",
                "        replays = list(REPLAY_DIR.glob(\"*.npz\"))\n",
                "        print(f\"\\nReplay batches: {len(replays)}\")\n",
                "        if replays:\n",
                "            total_size = sum(r.stat().st_size for r in replays) / (1024 * 1024)\n",
                "            print(f\"  Total size: {total_size:.2f} MB\")\n",
                "    \n",
                "    if torch.cuda.is_available():\n",
                "        print(\"\\nGPU Memory:\")\n",
                "        print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
                "        print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
                "\n",
                "show_training_status()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
